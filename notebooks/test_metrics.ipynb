{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from ir_eval.metrics import recall, precision, hole, ndcg\n",
    "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
    "from ir_eval.utils_prompt import load_prompt_text, eval_prompt, preprocess_prompt\n",
    "import collections\n",
    "import os\n",
    "import json\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trec covid dataset\n",
    "https://paperswithcode.com/dataset/trec-covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_dataset(\"BeIR/trec-covid\", 'corpus')['corpus']\n",
    "queries = load_dataset(\"BeIR/trec-covid\", 'queries')['queries']\n",
    "qrels = load_dataset(\"BeIR/trec-covid-qrels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['_id', 'title', 'text'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(example):\n",
    "    example['full_text'] = '[Title] ' + example['title'] + ' [TEXT] ' + example['text']\n",
    "    return example\n",
    "corpus = corpus.map(combine_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe94acfafa87431c960fc556ea3a58f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5355 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_embeddings = model.encode(corpus['full_text'], convert_to_tensor=True, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e9018baa8f4a3fa9cd4c22f7cb152f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_embeddings = model.encode(queries['text'], convert_to_tensor=True, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 384])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = util.semantic_search(query_embeddings, corpus_embeddings, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus['_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retrieval_results = collections.defaultdict(dict)\n",
    "doc_id_map = corpus['_id']\n",
    "query_id_map = queries['_id']\n",
    "for qid, doc_score_list in enumerate(hits):\n",
    "    qid_key = query_id_map[qid]\n",
    "    result_dict = {}\n",
    "    doc_ids = [doc_score['corpus_id'] for doc_score in doc_score_list]\n",
    "    scores = [doc_score['score'] for doc_score in doc_score_list]\n",
    "    \n",
    "    doc_id_keys = list(map(lambda x: doc_id_map[x], doc_ids))\n",
    "    result_dict = dict(zip(doc_id_keys, scores))\n",
    "    retrieval_results[qid_key] = result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels_for_eval = collections.defaultdict(dict)\n",
    "for example in qrels['test']:\n",
    "    qrels_for_eval[str(example['query-id'])][str(example['corpus-id'])] = example['score']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Recall@1': 0.00191, 'Recall@3': 0.00488, 'Recall@5': 0.00775, 'Recall@10': 0.01431, 'Recall@20': 0.01431, 'Recall@30': 0.01431, 'Recall@100': 0.01431, 'Recall@500': 0.01431, 'Recall@2000': 0.01431}\n",
      "{'Precision@1': 0.74, 'Precision@3': 0.64667, 'Precision@5': 0.612, 'Precision@10': 0.564, 'Precision@20': 0.282, 'Precision@30': 0.188, 'Precision@100': 0.0564, 'Precision@500': 0.01128, 'Precision@2000': 0.00282}\n",
      "{'NDCG@1': 0.67, 'NDCG@3': 0.60222, 'NDCG@5': 0.57071, 'NDCG@10': 0.53573}\n"
     ]
    }
   ],
   "source": [
    "print(recall(qrels=qrels_for_eval, results=retrieval_results, k_values=[1, 3, 5, 10, 20, 30, 100, 500, 2000]))\n",
    "print(precision(qrels=qrels_for_eval, results=retrieval_results, k_values=[1, 3, 5, 10, 20, 30, 100, 500, 2000]))\n",
    "print(ndcg(qrels=qrels_for_eval, results=retrieval_results, k_values=[1, 3, 5, 10]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4o reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1: prepare input dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_2_query = dict(zip(queries['_id'], queries['text']))\n",
    "docid_2_title = dict(zip(corpus['_id'], corpus['title']))\n",
    "docid_2_text = dict(zip(corpus['_id'], corpus['text']))\n",
    "docid_2_combined_text = dict(zip(corpus['_id'], [title + \" \" + text for title, text in zip(corpus['title'], corpus['text'])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_dicts = []\n",
    "for qid, doc_score_dict in retrieval_results.items():\n",
    "    query = qid_2_query[qid]\n",
    "    for docid, _ in doc_score_dict.items():\n",
    "        title = docid_2_title[docid]\n",
    "        text = docid_2_text[docid]\n",
    "        uid = qid + '@' + docid\n",
    "        \n",
    "        record = {'prompt_id': uid,\n",
    "                  'qid': qid,\n",
    "                  'docid': docid,\n",
    "                  'query': query,\n",
    "                  'title': title,\n",
    "                  'text': text\n",
    "                  }\n",
    "        all_input_dicts.append(record)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: populate prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_path = \"/mnt/d/Dropbox/llm_book/repos/ir_eval/prompts/query_doc_rating.jinja\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"gpt4_scoring.jsonl\"\n",
    "force_rerun = False\n",
    "model = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_inference(prompt_template_path, all_input_dicts, model, output_path):\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"output_path {output_path} exists\")\n",
    "        return\n",
    "    prompt_template_text = load_prompt_text(prompt_template_path)\n",
    "    for input_dict in all_input_dicts:\n",
    "        prompt_info_dict = preprocess_prompt(prompt_template_text, input_dict)\n",
    "        eval_results = eval_prompt(prompt_info_dict, model=model)\n",
    "        input_dict.update({f\"eval_result_{model}\": eval_results})\n",
    "        #break\n",
    "    \n",
    "    with open(output_path,\"w\") as json_file:\n",
    "        json.dump(all_input_dicts, json_file, indent=4)\n",
    "    return all_input_dicts\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = llm_inference(prompt_template_path, all_input_dicts, model, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse score and compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_score(text, tag='R'):\n",
    "    pattern = fr\"<{tag}>(.*?)</{tag}>\"  # Dynamic tag in regex\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    if matches:\n",
    "        return int(matches[0])\n",
    "    else:\n",
    "        print(\"score parsing failure\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [d.update({\"score\": parse_score(d['eval_result_gpt-4o-mini'])}) for d in result_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_results_gpt4 = collections.defaultdict(dict)\n",
    "doc_id_map = corpus['_id']\n",
    "query_id_map = queries['_id']\n",
    "for d in result_dict:\n",
    "    qid = d['qid']\n",
    "    doc_id = d['docid']\n",
    "    score = d['score']\n",
    "    retrieval_results_gpt4[qid][doc_id] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Recall@1': 0.00217, 'Recall@3': 0.00588, 'Recall@5': 0.00884, 'Recall@10': 0.01431}\n",
      "{'Precision@1': 0.8, 'Precision@3': 0.75333, 'Precision@5': 0.696, 'Precision@10': 0.564}\n",
      "{'NDCG@1': 0.72, 'NDCG@3': 0.68281, 'NDCG@5': 0.65106, 'NDCG@10': 0.56613}\n"
     ]
    }
   ],
   "source": [
    "# gpt4o mini result\n",
    "print(recall(qrels=qrels_for_eval, results=retrieval_results_gpt4, k_values=[1, 3, 5, 10]))\n",
    "print(precision(qrels=qrels_for_eval, results=retrieval_results_gpt4, k_values=[1, 3, 5, 10]))\n",
    "print(ndcg(qrels=qrels_for_eval, results=retrieval_results_gpt4, k_values=[1, 3, 5, 10]))\n",
    "\n",
    "# {'Recall@1': 0.00217, 'Recall@3': 0.00588, 'Recall@5': 0.00884, 'Recall@10': 0.01431}\n",
    "# {'Precision@1': 0.8, 'Precision@3': 0.75333, 'Precision@5': 0.696, 'Precision@10': 0.564}\n",
    "# {'NDCG@1': 0.72, 'NDCG@3': 0.68281, 'NDCG@5': 0.65106, 'NDCG@10': 0.56613}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Recall@1': 0.00191, 'Recall@3': 0.00488, 'Recall@5': 0.00775, 'Recall@10': 0.01431}\n",
      "{'Precision@1': 0.74, 'Precision@3': 0.64667, 'Precision@5': 0.612, 'Precision@10': 0.564}\n",
      "{'NDCG@1': 0.67, 'NDCG@3': 0.60222, 'NDCG@5': 0.57071, 'NDCG@10': 0.53573}\n"
     ]
    }
   ],
   "source": [
    "# biencoder result\n",
    "print(recall(qrels=qrels_for_eval, results=retrieval_results, k_values=[1, 3, 5, 10]))\n",
    "print(precision(qrels=qrels_for_eval, results=retrieval_results, k_values=[1, 3, 5, 10]))\n",
    "print(ndcg(qrels=qrels_for_eval, results=retrieval_results, k_values=[1, 3, 5, 10]))\n",
    "\n",
    "# biencoder result\n",
    "# print(recall(qrels=qrels_for_eval, results=retrieval_results, k_values=[1, 3, 5, 10]))\n",
    "# print(precision(qrels=qrels_for_eval, results=retrieval_results, k_values=[1, 3, 5, 10]))\n",
    "# print(ndcg(qrels=qrels_for_eval, results=retrieval_results, k_values=[1, 3, 5, 10]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Recall@1': 0.00267, 'Recall@3': 0.00802, 'Recall@5': 0.01337, 'Recall@10': 0.02674}\n",
      "{'Precision@1': 1.0, 'Precision@3': 1.0, 'Precision@5': 1.0, 'Precision@10': 1.0}\n",
      "{'NDCG@1': 1.0, 'NDCG@3': 1.0, 'NDCG@5': 1.0, 'NDCG@10': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# perfect rank result\n",
    "print(recall(qrels=qrels_for_eval, results=qrels_for_eval, k_values=[1, 3, 5, 10]))\n",
    "print(precision(qrels=qrels_for_eval, results=qrels_for_eval, k_values=[1, 3, 5, 10]))\n",
    "print(ndcg(qrels=qrels_for_eval, results=qrels_for_eval, k_values=[1, 3, 5, 10]))\n",
    "\n",
    "# {'Recall@1': 0.00267, 'Recall@3': 0.00802, 'Recall@5': 0.01337, 'Recall@10': 0.02674}\n",
    "# {'Precision@1': 1.0, 'Precision@3': 1.0, 'Precision@5': 1.0, 'Precision@10': 1.0}\n",
    "# {'NDCG@1': 1.0, 'NDCG@3': 1.0, 'NDCG@5': 1.0, 'NDCG@10': 1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-encoder reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_results = retrieval_results.copy()\n",
    "for qid, doc_score_dict in rerank_results.items():\n",
    "    query = qid_2_query[qid]\n",
    "    cross_inputs = []\n",
    "    doc_ids = []\n",
    "    for docid, _ in doc_score_dict.items():\n",
    "        \n",
    "        combined_text = docid_2_combined_text[docid]\n",
    "        cross_inputs.append([query, combined_text])\n",
    "        doc_ids.append(docid)\n",
    "    cross_encoder_scores = cross_encoder.predict(cross_inputs)\n",
    "    doc_score_dict = dict(zip(doc_ids,cross_encoder_scores.tolist()))\n",
    "    rerank_results[qid] = doc_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Recall@1': 0.002, 'Recall@3': 0.00548, 'Recall@5': 0.00858, 'Recall@10': 0.01431}\n",
      "{'Precision@1': 0.78, 'Precision@3': 0.74, 'Precision@5': 0.684, 'Precision@10': 0.564}\n",
      "{'NDCG@1': 0.72, 'NDCG@3': 0.67842, 'NDCG@5': 0.64002, 'NDCG@10': 0.56481}\n"
     ]
    }
   ],
   "source": [
    "# cross encoder result\n",
    "print(recall(qrels=qrels_for_eval, results=rerank_results, k_values=[1, 3, 5, 10]))\n",
    "print(precision(qrels=qrels_for_eval, results=rerank_results, k_values=[1, 3, 5, 10]))\n",
    "print(ndcg(qrels=qrels_for_eval, results=rerank_results, k_values=[1, 3, 5, 10]))\n",
    "\n",
    "# {'Recall@1': 0.002, 'Recall@3': 0.00548, 'Recall@5': 0.00858, 'Recall@10': 0.01431}\n",
    "# {'Precision@1': 0.78, 'Precision@3': 0.74, 'Precision@5': 0.684, 'Precision@10': 0.564}\n",
    "# {'NDCG@1': 0.72, 'NDCG@3': 0.67842, 'NDCG@5': 0.64002, 'NDCG@10': 0.56481}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Late-Interactive Biencoder\n",
    "Follow example here: https://github.com/lightonai/pylate?tab=readme-ov-file#retrieve\n",
    "Set up model and index. Then we can add documents to the index using their embeddings and corresponding ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sentence_transformers.SentenceTransformer:You try to use a model that was created with version 3.4.1, however, your version is 3.3.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "WARNING:pylate.models.colbert:PyLate model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from pylate import indexes, models, retrieve\n",
    "\n",
    "model = models.ColBERT(\n",
    "    model_name_or_path=\"lightonai/colbertv2.0\",\n",
    ")\n",
    "\n",
    "index = indexes.Voyager(\n",
    "    index_folder=\"pylate-index\",\n",
    "    index_name=\"index\",\n",
    "    override=True,\n",
    ")\n",
    "\n",
    "# \n",
    "retriever = retrieve.ColBERT(index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff7c08e123d4b17882d49d1641b2f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding documents (bs=32):   0%|          | 0/5355 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents to the index (bs=2000): 100%|██████████| 86/86 [47:54<00:00, 33.43s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pylate.indexes.voyager.Voyager at 0x7f3d1ce3f990>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Encode the documents\n",
    "documents_embeddings = model.encode(\n",
    "    list(docid_2_combined_text.values()),\n",
    "    batch_size=32,\n",
    "    is_query=False, # Encoding documents\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "\n",
    "# Add the documents ids and embeddings to the Voyager index\n",
    "index.add_documents(\n",
    "    documents_ids=list(docid_2_combined_text.keys()),\n",
    "    documents_embeddings=documents_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24baa4f43cc441f6bf5e5a824444148a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding queries (bs=32):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# query encoding\n",
    "queries_embeddings = model.encode(\n",
    "    list(qid_2_query.values()),\n",
    "    batch_size=32,\n",
    "    is_query=True, # Encoding queries\n",
    "    show_progress_bar=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving documents (bs=50):  50%|█████     | 1/2 [03:29<03:29, 209.70s/it]\n"
     ]
    }
   ],
   "source": [
    "scores = retriever.retrieve(\n",
    "    queries_embeddings=queries_embeddings, \n",
    "    k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "colbert_retrieval_results = collections.defaultdict(dict)\n",
    "for qid, doc_scores in zip(qid_2_query.keys(), scores):\n",
    "    for result in doc_scores:    \n",
    "        colbert_retrieval_results[qid][result['id']] = result['score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Recall@1': 0.00201, 'Recall@3': 0.00639, 'Recall@5': 0.01011, 'Recall@10': 0.01958}\n",
      "{'Precision@1': 0.86, 'Precision@3': 0.86, 'Precision@5': 0.816, 'Precision@10': 0.79}\n",
      "{'NDCG@1': 0.81, 'NDCG@3': 0.79041, 'NDCG@5': 0.76035, 'NDCG@10': 0.73834}\n"
     ]
    }
   ],
   "source": [
    "print(recall(qrels=qrels_for_eval, results=colbert_retrieval_results, k_values=[1, 3, 5, 10]))\n",
    "print(precision(qrels=qrels_for_eval, results=colbert_retrieval_results, k_values=[1, 3, 5, 10]))\n",
    "print(ndcg(qrels=qrels_for_eval, results=colbert_retrieval_results, k_values=[1, 3, 5, 10]))\n",
    "\n",
    "# {'Recall@1': 0.00201, 'Recall@3': 0.00639, 'Recall@5': 0.01011, 'Recall@10': 0.01958}\n",
    "# {'Precision@1': 0.86, 'Precision@3': 0.86, 'Precision@5': 0.816, 'Precision@10': 0.79}\n",
    "# {'NDCG@1': 0.81, 'NDCG@3': 0.79041, 'NDCG@5': 0.76035, 'NDCG@10': 0.73834}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface_lastest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
